{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data in Database Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3 as sq3\n",
    "import pandas.io.sql as pds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the database\n",
    "!wget -P ./ https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/classic_rock.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize path to SQLite databasejdbc:sqlite:/C:/__tmp/test/sqlite/jdbcTest.db\n",
    "path = 'data/classic_rock.db'\n",
    "con = sq3.Connection(path)\n",
    "\n",
    "# We now have a live connection to our SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've got a connection to our database, we can perform queries, and load their results in as Pandas DataFrames\n",
    "\n",
    "# Write the query\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM rock_songs;\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "observations = pds.read_sql(query, con)\n",
    "\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also run any supported SQL query\n",
    "# Write the query\n",
    "query = '''\n",
    "SELECT Artist, Release_Year, COUNT(*) AS num_songs, AVG(PlayCount) AS avg_plays  \n",
    "    FROM rock_songs\n",
    "    GROUP BY Artist, Release_Year\n",
    "    ORDER BY num_songs desc;\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "observations = pds.read_sql(query, con)\n",
    "\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common paramters that can be used to read in SQL data with formatting:\n",
    "\n",
    " - coerce_float: Attempt to force numbers into floats\n",
    " - parse_dates: List of columns to parse as dates\n",
    " - chunksize: Number of rows to include in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "SELECT Artist, Release_Year, COUNT(*) AS num_songs, AVG(PlayCount) AS avg_plays  \n",
    "    FROM rock_songs\n",
    "    GROUP BY Artist, Release_Year\n",
    "    ORDER BY num_songs desc;\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "observations_generator = pds.read_sql(query,\n",
    "                            con,\n",
    "                            coerce_float=True, # Doesn't efefct this dataset, because floats were correctly parsed\n",
    "                            parse_dates=['Release_Year'], # Parse `Release_Year` as a date\n",
    "                            chunksize=5 # Allows for streaming results as a series of shorter tables\n",
    "                           )\n",
    "\n",
    "for index, observations in enumerate(observations_generator):\n",
    "    if index < 5:\n",
    "        print(f'Observations index: {index}'.format(index))\n",
    "        display(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the database\n",
    "!wget -P ./ https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/baseball.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'baseball.db'\n",
    "\n",
    "# Create a connection, `con`, that is connected to database at `path`\n",
    "con = sq3.Connection(path)\n",
    "\n",
    "# Create a variable, `query`, containing a SQL query which reads in all data from the `` table\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "    FROM allstarfull\n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "allstar_observations = pd.read_sql(query, con)\n",
    "\n",
    "# print(allstar_observations)\n",
    "\n",
    "# Create a variable, tables, which reads in all data from the table sqlite_master\n",
    "all_tables = pd.read_sql('SELECT * FROM sqlite_master', con)\n",
    "print(all_tables)\n",
    "\n",
    "# Pretend that you were interesting in creating a new baseball hall of fame. Join and analyze the tables to evaluate the top 3 all time best baseball players\n",
    "best_query = \"\"\"\n",
    "SELECT playerID, sum(GP) AS num_games_played, AVG(startingPos) AS avg_starting_position\n",
    "    FROM allstarfull\n",
    "    GROUP BY playerID\n",
    "    ORDER BY num_games_played DESC, avg_starting_position ASC\n",
    "    LIMIT 3\n",
    "\"\"\"\n",
    "best = pd.read_sql(best_query, con)\n",
    "print(best.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/Ames_Housing_Data1.tsv'\n",
    "housing = pd.read_csv(path, sep='\\t')\n",
    "\n",
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find more information about the features and types using the `info()`  method.\n",
    "\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the output above, we have 2930 entries, 0 to 2929, as well as 81 features. The \"Non-Null Count\" column shows the number of non-null entries.  If the count is 2930 then there is no missing values for that particular feature. 'SalePrice' is our target or response variable and the rest of the features are our predictor variables.\n",
    "\n",
    "We also have a mix of numerical (28 int64 and 11 float64) and object data types. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, it is important to note that the minimum value is greater than 0. Also, there is a big difference between the minimum value and the 25th percentile. It is bigger than the 75th percentile and the maximum value. This means that our data might not be normally distributed (an important assumption for linear regression analysis), so will check for normality in the Log Transform section. \n",
    "\n",
    "The `describe()` function reveals the statistical information about the numeric attributes. To reveal some information about our categorical (object) attributes, we can use `value_counts()` function. In this exercise, describe all categories of the 'Sale Condition' attribute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"Sale Condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looking for Correlations**\n",
    "\n",
    "Before starting the process of data cleaning, it is important to determine the correlation between the response variable (in our case, the sale price) and other predictor variables. Some variables may not have a significant impact on the price of the house and will not be included in the analysis. There are several methods to identify the correlation between the target variable and other features, such as pair plots, scatter plots, heat maps, and correlation matrices. In this case, we will use the `corr()` function to list the top features based on the Pearson correlation coefficient, which measures how closely two sequences of numbers are correlated. However, it's important to note that correlation coefficient can only be calculated on numerical attributes like floats and integers. Therefore, only numeric attributes will be selected for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hous_num = housing.select_dtypes(include = ['float64', 'int64'])\n",
    "hous_num_corr = hous_num.corr()['SalePrice'][:-1] # -1 means that the latest row is SalePrice\n",
    "\n",
    "# print(hous_num_corr )\n",
    "top_features = hous_num_corr[abs(hous_num_corr) > 0.5].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5\n",
    "print(f\"There is {len(top_features)} strongly correlated values with SalePrice:\\n{top_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, hous_num.columns.size, 5):\n",
    "    sns.pairplot(data=hous_num,\n",
    "                x_vars=hous_num.columns[i:i+5],\n",
    "                y_vars=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Log Transformation**\n",
    "In this section, we are going to inspect whether our 'SalePrice' data are normally distributed. The assumption of the normal distribution must be met in order to perform any type of regression analysis. There are several ways to check for this assumption, however here, we will use the visual method, by plotting the 'SalePrice' distribution using the `distplot()` function from the `seaborn` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_untransformed = sns.distplot(housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The range of skewness for a fairly symmetrical bell curve distribution is between -0.5 and 0.5; moderate skewness is -0.5 to -1.0 and 0.5 to 1.0; and highly skewed distribution is < -1.0 and > 1.0. In our case, we have ~1.7, so it is considered  highly skewed data. \n",
    "\n",
    "print(f\"Skewness: {housing['SalePrice'].skew()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our data, so it looks more normally distributed.\n",
    "log_transformed = np.log(housing['SalePrice'])\n",
    "\n",
    "sp_transformed = sns.distplot(log_transformed)\n",
    "\n",
    "print(f\"Skewness: {log_transformed.skew()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to correct for skewness of the data are: Square Root Transform (`np.sqrt`) and the Box-Cox Transform (`stats.boxcox` from the `scipy stats` library).\n",
    "\n",
    "## **Handling the Duplicates**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find duplicate rows\n",
    "duplicate = housing[housing.duplicated(['PID'])]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_removed = housing.drop_duplicates()\n",
    "dup_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to check if there are any duplicated Indexes in our dataset is to use the `is_unique` method.\n",
    "housing.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling the Missing Values**\n",
    "To facilitate the identification of any missing values, pandas offers three functions: `isna()`, `isnull()`, and `notna()`.\n",
    "\n",
    "To get an overview of the missing values in our dataset, we will use the `isnull()` function. Then, we will sum up the values using the `sum()` function, sort them with `sort_values()`, and finally plot the first 20 columns, which have the highest number of missing values, using the `bar plot` function from the `matplotlib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = housing.isnull().sum().sort_values(ascending=False)\n",
    "total_select = total.head(20)\n",
    "total_select.plot(kind=\"bar\", figsize = (8,6), fontsize = 10)\n",
    "\n",
    "plt.xlabel(\"Columns\", fontsize = 20)\n",
    "plt.ylabel(\"Count\", fontsize = 20)\n",
    "plt.title(\"Total Missing Values\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the rows, containing null values in 'Lot Frontage' feature will be dropped.\n",
    "housing.dropna(subset=[\"Lot Frontage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the whole attribute or feature (column), that contains missing values:\n",
    "housing.drop(\"Lot Frontage\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the missing values (zero, the mean, the median, etc.)\n",
    "median = housing[\"Lot Frontage\"].median()\n",
    "housing[\"Lot Frontage\"].fillna(median, inplace = True)\n",
    "housing.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Scaling**\n",
    "Feature scaling is a crucial step in data transformation. Two common techniques include min-max scaling and standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = MinMaxScaler().fit_transform(hous_num)\n",
    "norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(hous_num)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling the Outliers**\n",
    "\n",
    "In statistics, an outlier refers to an observation that is significantly different from other observations. This can happen due to errors in data collection or recording, or simply because of natural variability in the data. The way we treat an outlier largely depends on the type of analysis we want to perform and the data we are working with. Outliers can have a major impact on our statistical models, and can also provide valuable insights into specific behaviours.\n",
    "\n",
    "There are many techniques to discover outliers in our data, such as Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the easiest ways to detect an outlier is to visually inspect the data using box plots or scatter plots.\n",
    "\n",
    "### Uni-variate Analysis\n",
    "From the two plots shown, it is evident that there are some points outside the box plot area that significantly deviate from the rest of the population. The decision to keep or remove them depends on how well we understand our data and the type of analysis to be performed. In this particular case, the points outside the box plot area for 'Lot Area' and 'Sale Price' may represent actual true data points, and therefore, they should not be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=housing['Lot Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-variate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_area = housing.plot.scatter(x='Gr Liv Area',\n",
    "                      y='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.sort_values(by = 'Gr Liv Area', ascending = False)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_dropped = housing.drop(housing.index[[1499,2181]])\n",
    "\n",
    "new_plot = outliers_dropped.plot.scatter(x='Gr Liv Area',\n",
    "                                         y='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score Analysis\n",
    "\n",
    "Z-score can be used to identify outliers in a mathematical way. It is calculated by measuring the number of standard deviations by which an observation or data point is above or below the mean value of the group being observed. In other words, the Z-score is a value that measures the relationship between a specific data point and the mean and standard deviation of the entire group. Data points that are too far from the mean, typically more than 3 or less than -3 standard deviations, are considered outliers. Therefore, if the Z-score value of a data point exceeds this threshold, it will be identified as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['LQFSF_Stats'] = stats.zscore(housing['Low Qual Fin SF'])\n",
    "housing[['Low Qual Fin SF','LQFSF_Stats']].describe().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max value of 22.882 is further proof of the presence of outliers, as it falls well above the z-score limit of +3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/iris_data.csv'\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows\n",
    "print(data.shape)\n",
    "\n",
    "# Column names\n",
    "print(data.columns.tolist())\n",
    "\n",
    "# Data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = data.describe()\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.loc['range'] = stats_df.loc['max'] - stats_df.loc['min']\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fields = ['mean','25%','50%','75%', 'range']\n",
    "stats_df = stats_df.loc[out_fields]\n",
    "stats_df.rename({'50%': 'median'}, inplace=True)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('species').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying multiple functions at once - 2 methods\n",
    "\n",
    "data.groupby('species').agg(['mean', 'median'])  # passing a list of recognized strings\n",
    "data.groupby('species').agg([np.mean, np.median])  # passing a list of explicit aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "agg_dict = {field: ['mean', 'median'] for field in data.columns if field != 'species'}\n",
    "agg_dict['petal_length'] = 'max'\n",
    "pprint(agg_dict)\n",
    "data.groupby('species').agg(agg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(data.sepal_length, data.sepal_width)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Sepal Length (cm)',\n",
    "       ylabel='Sepal Width (cm)',\n",
    "       title='Sepal Length vs Width');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "ax.hist(data.petal_length, bins=25);\n",
    "\n",
    "ax.set(xlabel='Petal Length (cm)', \n",
    "       ylabel='Frequency',\n",
    "       title='Distribution of Petal Lengths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook')\n",
    "data.plot.hist(bins=25, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create four separate plots, use Pandas `.hist` method\n",
    "sns.set_context('talk')\n",
    "axList = data.hist(bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create four separate plots, use Pandas `.hist` method\n",
    "sns.set_context('notebook')\n",
    "\n",
    "axList = data.hist(bins=25)\n",
    "\n",
    "# Add some x- and y- labels to first column and last row\n",
    "for ax in axList.flatten():\n",
    "    if ax.is_last_row():\n",
    "        ax.set_xlabel('Size (cm)')\n",
    "        \n",
    "    if ax.is_first_col():\n",
    "        ax.set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_context('poster')\n",
    "\n",
    "data.boxplot(by='species');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to reshape the data so there is \n",
    "# only a single measurement in each column\n",
    "\n",
    "plot_data = (data\n",
    "             .set_index('species')\n",
    "             .stack()\n",
    "             .to_frame()\n",
    "             .reset_index()\n",
    "             .rename(columns={0:'size', 'level_1':'measurement'})\n",
    "            )\n",
    "\n",
    "plot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "f = plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='measurement', y='size', \n",
    "            hue='species', data=plot_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "sns.pairplot(data, hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the Ames Housing Data\n",
    "path = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/Ames_Housing_Data.tsv'\n",
    "\n",
    "df = pd.read_csv(path, sep='\\t')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is recommended by the data set author to remove a few outliers\n",
    "\n",
    "df = df.loc[df['Gr Liv Area'] <= 4000,:]\n",
    "print(\"Number of rows in the data:\", df.shape[0])\n",
    "print(\"Number of columns in the data:\", df.shape[1])\n",
    "data = df.copy() # Keep a copy our original data \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that are not useful for ML and for which the value is unique for all observations\n",
    "df.drop(['Order', 'PID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Pd.Series consisting of all the string categoricals\n",
    "one_hot_encode_cols = df.dtypes[df.dtypes == object]  # filtering by string categoricals\n",
    "one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n",
    "\n",
    "df[one_hot_encode_cols].head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the one hot encoding\n",
    "df = pd.get_dummies(df, columns=one_hot_encode_cols, drop_first=True)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of float colums to check for skewing\n",
    "mask = data.dtypes == float\n",
    "float_cols = df.select_dtypes('number').columns #data.columns[mask]\n",
    "\n",
    "skew_limit = 0.75 # define a limit above which we will log transform\n",
    "skew_vals = data[float_cols].skew()\n",
    "print(skew_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the skewed columns\n",
    "skew_cols = (skew_vals\n",
    "             .sort_values(ascending=False)\n",
    "             .to_frame()\n",
    "             .rename(columns={0:'Skew'})\n",
    "             .query('abs(Skew) > {}'.format(skew_limit)))\n",
    "\n",
    "skew_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
    "\n",
    "# Choose a field\n",
    "field = \"SalePrice\"\n",
    "\n",
    "# Create two \"subplots\" and a \"figure\" using matplotlib\n",
    "fig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Create a histogram on the \"ax_before\" subplot\n",
    "df[field].hist(ax=ax_before)\n",
    "\n",
    "# Apply a log transformation (numpy syntax) to this column\n",
    "df[field].apply(np.log1p).hist(ax=ax_after)\n",
    "\n",
    "# Formatting of titles etc. for each subplot\n",
    "ax_before.set(title='before np.log1p', ylabel='frequency', xlabel='value')\n",
    "ax_after.set(title='after np.log1p', ylabel='frequency', xlabel='value')\n",
    "fig.suptitle('Field \"{}\"'.format(field));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the skew transformation:\n",
    "\n",
    "for col in skew_cols.index.values:\n",
    "    if col == \"SalePrice\":\n",
    "        continue\n",
    "    df[col] = df[col].apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have a larger set of potentially-useful features\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a *lot* of variables. Let's go back to our saved original data and look at how many values are missing for each variable. \n",
    "df = data\n",
    "data.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars','SalePrice']]\n",
    "\n",
    "# Now we can look at summary statistics of the subset data\n",
    "smaller_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There appears to be one NA in Garage Cars - we will take a simple approach and fill it with 0\n",
    "smaller_df = smaller_df.fillna(0)\n",
    "\n",
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate our features from our target\n",
    "\n",
    "X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars']]\n",
    "\n",
    "y = smaller_df['SalePrice']\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['House Style'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['House Style'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_counts = df.Neighborhood.value_counts()\n",
    "nbh_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_nbhs = list(nbh_counts[nbh_counts <= 8].index)\n",
    "\n",
    "other_nbhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = X.copy()\n",
    "\n",
    "X4['Neighborhood'] = df['Neighborhood'].replace(other_nbhs, 'Other')\n",
    "\n",
    "X4.Neighborhood.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
